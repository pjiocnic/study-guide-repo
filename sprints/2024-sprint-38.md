
# Python

# [Learn Python On AWS Workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/3d705026-9edc-40e8-b353-bdabb116c89c/en-US)
1. [Learn Python AWS Workshop— 01. Setup Cloud9 IDE, Python Warm-up, Install & Test AWS Boto3 Package](https://medium.com/the-aws-way/learn-python-aws-workshop-01-setup-cloud9-ide-python-warm-up-install-test-aws-boto3-package-90ce0b465aab)
2. [Learn Python AWS Workshop — 02. Functions, Arguments/Parameters, Loops, If Statement w/ Boto3 Demo by ](https://medium.com/the-aws-way/learn-python-aws-workshop-02-functions-arguments-parameters-loops-if-statement-w-boto3-demo-dc480b0fd32b)

# Curate Python

https://community.aws/content/2ZSHJZNvXOjrUUOjK3SRGfK2Krc/teach-yourself-python-in-2024?lang=en
https://aws.amazon.com/developer/language/python/
https://aws.amazon.com/developer/code-examples/?code-examples-all.sort-by=item.additionalFields.category&code-examples-all.sort-order=desc&awsf.sdk-code-examples-programming-language=programming-language%23python&awsf.sdk-code-examples-type=*all&awsf.sdk-code-samples-tech-category=*all&awsf.sdk-code-examples-product=*all


# Blogs To Read

1. [Sending and receiving CloudEvents with Amazon EventBridge by David Boyne](https://aws.amazon.com/blogs/compute/sending-and-receiving-cloudevents-with-amazon-eventbridge/)
1. https://docs.aws.amazon.com/prescriptive-guidance/latest/modernization-data-persistence/welcome.html
1. [How to centralize hybrid DNS in AWS by Cedric De Schepper](https://awstip.com/how-to-centralise-hybrid-dns-in-aws-ea8b6da9609c)

# Patterns to collect

1. Aggregate files coming in out of sequence .. does Step function workshop have this?
1. https://beam.apache.org/documentation/patterns/file-processing/
2. Refresh this page - /Volumes/Lexar/git-repos/aws-repo/my-github/study-guide-repo/amazon/aws-patterns.md

# Amazon Q says for "Aggregating files that arrive at different times"

Aggregating files that arrive at different times is indeed a common challenge in data processing pipelines. Here's an overview of approaches and considerations for this scenario:

**Event-driven architecture:**

Use services like S3 event notifications or Amazon EventBridge to trigger processing as files arrive.

Each file triggers a Lambda function for initial processing.

Processed results are stored in a temporary location (e.g., S3 bucket or DynamoDB table).

**State management:**

Maintain a state store (e.g., DynamoDB table) to track which files have been processed.

Record metadata such as file names, timestamps, and processing status.

**Aggregation strategies:**

**Time-based:** Aggregate files that arrive within a specific time window.

**Count-based:** Wait for a predetermined number of files before aggregating.

**Condition-based:** Aggregate when a specific condition is met (e.g., arrival of a "completion" file).

**AWS Step Functions for orchestration:**

Create a state machine to manage the overall workflow.

Use Wait states to handle delays between file arrivals.

Implement Choice states to decide when to proceed with aggregation.

**Parallel processing:**

Use Step Functions' Map state or AWS Batch for parallel file processing.

This is especially useful for large numbers of files or compute-intensive processing.

Here's a simplified Python example of how you might implement part of this logic using AWS Lambda and DynamoDB:

```py
import boto3
import os
from datetime import datetime, timedelta

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table(os.environ['STATE_TABLE'])

def lambda_handler(event, context):
    file_name = event['Records'][0]['s3']['object']['key']
    process_file(file_name)
    update_state(file_name)

    if should_aggregate():
        trigger_aggregation()

def process_file(file_name):
    # Process the file
    pass

def update_state(file_name):
    table.put_item(
        Item={
            'file_name': file_name,
            'processed_at': datetime.now().isoformat(),
            'status': 'processed'
        }
    )

def should_aggregate():
    # Check if it's time to aggregate
    cutoff_time = datetime.now() - timedelta(hours=1)
    response = table.scan(
        FilterExpression='processed_at > :cutoff',
        ExpressionAttributeValues={':cutoff': cutoff_time.isoformat()}
    )
    return len(response['Items']) >= 10  # Aggregate after 10 files or 1 hour

def trigger_aggregation():
    # Start the aggregation process
    pass
```

**This example demonstrates:**

Processing individual files as they arrive [1]

Updating a state store (DynamoDB) with file metadata

Checking if aggregation conditions are met (based on time and file count)

Triggering the aggregation process when conditions are met

To implement this architecture more robustly, you would typically use AWS Step Functions to orchestrate the overall process, including waiting for files, parallel processing, and managing the aggregation step.

Remember to consider error handling, retries, and monitoring in your actual implementation. Also, the specific aggregation logic will depend on your use case and data format.

**Sources**
[1] File processing patterns: [beam.apache.org > documentation > patterns >  file-processing](https://beam.apache.org/documentation/patterns/file-processing/)
