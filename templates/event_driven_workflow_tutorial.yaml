AWSTemplateFormatVersion: 2010-09-09
Description: AWS Glue tutorial for event driven workflow

Parameters:
  S3BucketName:
    Type: String
    Description: This is used to store data, CloudTrail logs, job scripts, and any temporary files generated during the AWS Glue ETL job run.

  WorkflowName:
    Type: String
    Description: A data processing pipeline that is comprised of a crawler, jobs, and triggers. This workflow converts uploaded data files into Apache Parquet format.
    Default: s3trigger_data_conversion_workflow

  DatabaseName:
    Type: String
    Description: The AWS Glue Data Catalog database that is used to hold the tables created in this walkthrough.
    Default: event_driven_workflow_tutorial

  TableName:
    Type: String
    Description: The Data Catalog table representing the Parquet files being converted by the workflow.
    Default: products

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: Configuration
        Parameters:
          - WorkflowName
          - S3BucketName
          - DatabaseName
          - TableName

Resources:
  Trail:
    DependsOn: S3Bucket
    Type: AWS::CloudTrail::Trail
    Properties:
      TrailName: !Sub 's3-event-trail-${AWS::StackName}'
      IsLogging: True
      S3BucketName: !Ref S3BucketName
      S3KeyPrefix: cloudtrail
      EventSelectors:
        - DataResources:
            - Type: AWS::S3::Object
              Values:
              - !Sub 'arn:aws:s3:::${S3BucketName}/data/products_raw/'
          IncludeManagementEvents: False
          ReadWriteType: WriteOnly

  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref S3BucketName
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True

  S3BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref S3BucketName
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - cloudtrail.amazonaws.com
            Action:
              - s3:GetBucketAcl
            Resource: !Sub arn:aws:s3:::${S3BucketName}
          - Effect: Allow
            Principal:
              Service:
                - cloudtrail.amazonaws.com
            Action:
              - s3:PutObject
            Resource: !Sub arn:aws:s3:::${S3BucketName}/cloudtrail/*
            Condition:
              StringEquals:
                s3:x-amz-acl: bucket-owner-full-control

  EventDrivenWorkflow:
    Type: AWS::Glue::Workflow
    Properties:
      Name: !Ref WorkflowName
      Description: Glue workflow triggered by S3 PutObject Event

  PreJobTrigger:
    DependsOn: PreJob
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Sub '${WorkflowName}_pre_job_trigger'
      Description: Glue trigger which is listening on S3 PutObject events
      Type: EVENT
      Actions:
        - JobName: !Ref PreJob
      WorkflowName: !Ref EventDrivenWorkflow

  CrawlerTrigger:
    DependsOn: Crawler
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Sub '${WorkflowName}_crawler_trigger'
      Description: Glue trigger which is listening on pre job completion
      Type: CONDITIONAL
      StartOnCreation: True
      Actions:
        - CrawlerName: !Ref Crawler
      Predicate:
        Logical: ANY
        Conditions:
          - LogicalOperator: EQUALS
            JobName: !Ref PreJob
            State: SUCCEEDED
      WorkflowName: !Ref EventDrivenWorkflow

  ConversionJobTrigger:
    DependsOn:
      - Crawler
      - ConversionJob
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Sub '${WorkflowName}_conversion_job_trigger'
      Description: Glue trigger which is listening on crawler completion
      Type: CONDITIONAL
      StartOnCreation: True
      Actions:
        - JobName: !Ref ConversionJob
      Predicate:
        Logical: ANY
        Conditions:
          - LogicalOperator: EQUALS
            CrawlerName: !Ref Crawler
            CrawlState: SUCCEEDED
      WorkflowName: !Ref EventDrivenWorkflow

  PostJobTrigger:
    DependsOn: PostJob
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Sub '${WorkflowName}_post_job_trigger'
      Description: Glue trigger which is listening on conversion job completion
      Type: CONDITIONAL
      StartOnCreation: True
      Actions:
        - JobName: !Ref PostJob
      Predicate:
        Logical: ANY
        Conditions:
          - LogicalOperator: EQUALS
            JobName: !Ref ConversionJob
            State: SUCCEEDED
      WorkflowName: !Ref EventDrivenWorkflow

  EventBridgeRule:
    DependsOn:
      - EventBridgeGlueExecutionRole
      - EventDrivenWorkflow
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub s3_file_upload_trigger_rule-${AWS::StackName}
      EventPattern:
        source:
          - aws.s3
        detail-type:
          - AWS API Call via CloudTrail
        detail:
          eventSource:
            - s3.amazonaws.com
          eventName:
            - PutObject
          requestParameters:
            bucketName:
              - !Ref S3BucketName
            key:
              - prefix: data/products_raw/
      Targets:
        -
          Arn: !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:workflow/${EventDrivenWorkflow}
          Id: CloudTrailTriggersWorkflow
          RoleArn: !GetAtt 'EventBridgeGlueExecutionRole.Arn'

  EventBridgeGlueExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub EventBridgeGlueExecutionRole-${AWS::StackName}
      Description: Has permissions to invoke the NotifyEvent API for an AWS Glue workflow.
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - events.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /

  GlueNotifyEventPolicy:
    DependsOn:
      - EventBridgeGlueExecutionRole
      - EventDrivenWorkflow
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: !Sub GlueNotifyEventPolicy-${AWS::StackName}
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          -
            Effect: "Allow"
            Action:
              - glue:notifyEvent
            Resource: !Sub arn:aws:glue:${AWS::Region}:${AWS::AccountId}:workflow/${EventDrivenWorkflow}
      Roles:
        - !Ref EventBridgeGlueExecutionRole

  GlueServiceRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub GlueServiceRole-${AWS::StackName}
      Description: Runs the AWS Glue job that has permission to download the script, read data from the source, and write data to the destination after conversion.
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole

  S3DataPolicy:
    DependsOn:
      - GlueServiceRole
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: !Sub S3DataPolicy-${AWS::StackName}
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          -
            Effect: "Allow"
            Action:
              - s3:GetObject
              - s3:PutObject
            Resource: !Sub arn:aws:s3:::${S3BucketName}/*
          -
            Effect: "Allow"
            Action:
              - s3:ListBucket
            Resource: !Sub arn:aws:s3:::${S3BucketName}
      Roles:
        - !Ref GlueServiceRole

  Database:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Ref DatabaseName
        Description: This database is used to organize the metadata tables created in this tutorial.

  SourceTable:
    DependsOn: Database
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref DatabaseName
      TableInput:
        Name: !Sub 'source_${TableName}'
        Description: This AWS Glue Data Catalog table has metadata information about the source data.
        TableType: EXTERNAL_TABLE
        StorageDescriptor:
          Location: !Sub 's3://${S3BucketName}/data/products_raw/'

  Crawler:
    DependsOn:
      - SourceTable
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${WorkflowName}_discovery_crawler'
      Description: Glue crawler which discovers source table schema
      DatabaseName: !Ref DatabaseName
      Role: !GetAtt GlueServiceRole.Arn
      SchemaChangePolicy:
        UpdateBehavior: UPDATE_IN_DATABASE
        DeleteBehavior: LOG
      Targets:
        CatalogTargets:
          - DatabaseName: !Ref DatabaseName
            Tables:
            - !Ref SourceTable
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableGroupingPolicy\":\"CombineCompatibleSchemas\"}}"

  PreJob:
    DependsOn:
      - S3CustomResource
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub '${WorkflowName}_pre_job'
      Description: Glue job that updates state to STARTED in workflow run properties
      Role: !Ref GlueServiceRole
      GlueVersion: 1.0
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: !Sub 's3://${S3BucketName}/script/update_workflow_property.py'
      MaxCapacity: 0.0625
      ExecutionProperty:
        MaxConcurrentRuns: 1
      DefaultArguments:
        --job-language: python
        --TempDir: !Sub 's3://${S3BucketName}/tmp/'
        --transition_state: STARTED
        --extra-py-files: !Sub 's3://aws-glue-assets-${AWS::Region}/scripts/lib/utils.py'

  PostJob:
    DependsOn:
      - S3CustomResource
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub '${WorkflowName}_post_job'
      Description: Glue job that updates workflow run property
      Role: !Ref GlueServiceRole
      GlueVersion: 1.0
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: !Sub 's3://${S3BucketName}/script/update_workflow_property.py'
      MaxCapacity: 0.0625
      ExecutionProperty:
        MaxConcurrentRuns: 1
      DefaultArguments:
        --job-language: python
        --TempDir: !Sub 's3://${S3BucketName}/tmp/'
        --transition_state: COMPLETED
        --extra-py-files: !Sub 's3://aws-glue-assets-${AWS::Region}/scripts/lib/utils.py'

  ConversionJob:
    DependsOn:
      - Database
      - SourceTable
      - S3CustomResource
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub '${WorkflowName}_conversion_etl_job'
      Description: Glue job that converts input data files into parquet format.
      Role: !Ref GlueServiceRole
      GlueVersion: 2.0
      Command:
        Name: glueetl
        PythonVersion: 3
        ScriptLocation: !Sub 's3://${S3BucketName}/script/conversion.py'
      NumberOfWorkers: 2
      WorkerType: G.1X
      ExecutionProperty:
        MaxConcurrentRuns: 1
      DefaultArguments:
        --job-bookmark-option: job-bookmark-enable
        --job-language: python
        --TempDir: !Sub 's3://${S3BucketName}/tmp/'
        --output_database: !Ref DatabaseName
        --tmp_table: !Ref SourceTable
        --output_table: !Ref TableName
        --output_path: !Sub 's3://${S3BucketName}/data/products/'

  S3CustomResource:
    DependsOn: S3Bucket
    Type: Custom::S3CustomResource
    Properties:
      ServiceToken: !GetAtt LambdaFunction.Arn
      the_bucket: !Ref S3BucketName
      the_conversion_script_file_key: script/conversion.py
      the_workflow_script_file_key: script/update_workflow_property.py
      the_origin_conversion_script_url: https://raw.githubusercontent.com/awslabs/aws-glue-blueprint-libs/master/samples/conversion/conversion.py
      the_origin_workflow_script_url: https://aws-bigdata-blog.s3.amazonaws.com/artifacts/glue_event_driven_workflow/update_workflow_property.py

  LambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: !Sub 'lambda-custom-resource-${AWS::StackName}'
      Description: This is used as an AWS CloudFormation custom resource to copy job scripts from an AWS Glue managed GitHub repository and an AWS Big Data blog  S3 bucket to your S3 bucket.
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 360
      Runtime: python3.8
      Code:
        ZipFile: !Sub
          - |
            import boto3
            from botocore.client import ClientError
            import cfnresponse
            import urllib.request
            def handler(event, context):
                # Init
                the_event = event['RequestType'].strip()
                print("The event is: ", the_event)
                response_data = {}
                s3 = boto3.client('s3')
                s3_resource = boto3.resource('s3')
                # Retrieve parameters
                the_bucket = event['ResourceProperties']['the_bucket'].strip()
                the_conversion_script_file_key = event['ResourceProperties']['the_conversion_script_file_key'].strip()
                the_workflow_script_file_key = event['ResourceProperties']['the_workflow_script_file_key'].strip()
                the_origin_conversion_script_url = event['ResourceProperties']['the_origin_conversion_script_url'].strip()
                the_origin_workflow_script_url = event['ResourceProperties']['the_origin_workflow_script_url'].strip()
                try:
                    if the_event in ('Create', 'Update'):
                        # Copying job script
                        try:
                            reqC = urllib.request.Request(the_origin_conversion_script_url, method='GET')
                            urlDataC = urllib.request.urlopen(reqC).read().decode('utf-8')
                            objC = s3_resource.Object(the_bucket,the_conversion_script_file_key)
                            objC.put(Body = urlDataC)
                            reqW = urllib.request.Request(the_origin_workflow_script_url, method='GET')
                            urlDataW = urllib.request.urlopen(reqW).read().decode('utf-8')
                            objW = s3_resource.Object(the_bucket,the_workflow_script_file_key)
                            objW.put(Body = urlDataW)
                        except ClientError as ce:
                            print("Failed to copy the source code file.")
                            print(ce)
                            print(ce.response['ResponseMetadata'])
                        except urllib.error.HTTPError as e:
                            print(e)
                    elif the_event == 'Delete':
                        try:
                          pages = []
                          paginator = s3.get_paginator('list_objects_v2')
                          for page in paginator.paginate(Bucket=the_bucket):
                            pages.extend(page['Contents'])
                          for source in pages:
                            s3.delete_object(Bucket=the_bucket,Key=source["Key"])
                        except ClientError as ce:
                            print(f"Failed to delete the files: {ce}")

                    # Everything OK... send the signal back
                    print("Completed.")
                    cfnresponse.send(event,
                                      context,
                                      cfnresponse.SUCCESS,
                                      response_data)
                except Exception as e:
                    print("Failed...")
                    print(str(e))
                    response_data['Data'] = str(e)
                    cfnresponse.send(event,
                                      context,
                                      cfnresponse.FAILED,
                                      response_data)
          - Region: !Ref AWS::Region

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub LambdaExecutionRole-${AWS::StackName}
      Description: Runs the Lambda function that has permission to upload the job scripts to the S3 bucket.
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: '2012-10-17'
      Path: "/"
      Policies:
        - PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Effect: Allow
                Resource: arn:aws:logs:*:*:*
          PolicyName: !Sub AWSLambda-CW-${AWS::StackName}
        - PolicyDocument:
            Version: '2012-10-17'
            Statement:
              -
                Effect: "Allow"
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
              -
                Effect: "Allow"
                Action:
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}
          PolicyName: !Sub AWSLambda-S3-${AWS::StackName}
