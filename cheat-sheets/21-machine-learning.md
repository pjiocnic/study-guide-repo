In machine learning, when you cleanse the data and standardize it, the process is commonly referred to as "preprocessing" or "data preprocessing".

Data preprocessing involves various steps such as:

Data Cleaning: Removing or correcting any inaccuracies, inconsistencies, or missing values in the dataset.
Data Standardization: Standardizing the scale of the features or variables in the dataset. This often involves techniques such as normalization or standardization to bring all features to a similar scale.
Feature Scaling: Scaling the features to ensure they have similar ranges. This can help algorithms converge faster and perform better.
Feature Engineering: Creating new features or transforming existing ones to improve the performance of the machine learning model.
Data Transformation: Transforming categorical variables into numerical ones (encoding), dealing with outliers, handling skewness, etc.
Standardization specifically refers to the process of rescaling the features so that they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1. This is achieved by subtracting the mean and dividing by the standard deviation of each feature.

Overall, data preprocessing is a crucial step in machine learning pipelines to ensure that the data is in a suitable format for training models and extracting meaningful insights.